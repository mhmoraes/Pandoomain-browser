include: "rules/globals.smk"
include: "rules/download.smk"


rule all:
    input:
        f"{RESULTS}/absence_presence.tsv",
        # Browser visualizer files
        f"{RESULTS}/browser_files/iscan.db",
        f"{RESULTS}/browser_files/metadata.db",
        f"{RESULTS}/browser_files/neighbors.db",


rule hmmer:
    input:
        f"{RESULTS}/genomes/genomes.tsv",
    output:
        hmmer=ensure(f"{RESULTS}/hmmer.tsv", non_empty=True),
    params:
        queries=f"{IN_QUERIES}",
    shell:
        r"""
workflow/scripts/hmmer.py {params} {input} {output}
"""


rule get_neighbors:
    input:
        hmmer=rules.hmmer.output,
    output:
        neighbors=ensure(f"{RESULTS}/neighbors.tsv", non_empty=True),
    threads: workflow.cores
    params:
        N=N_NEIGHBORS,
        gdir=f"{RESULTS}/genomes",  # Se convertira en output
    shell:
        """
workflow/scripts/neighbors.R {threads} {params} {input} >| {output}
"""


rule all_faa:
    input:
        neighbors=rules.get_neighbors.output,
    output:
        faa=f"{RESULTS}/all.faa",
    params:
        faa_width=FAA_WIDTH,
        db=f"{RESULTS}/genomes",
    threads: workflow.cores
    shell:
        r"""
workflow/scripts/harvest.R {params} {threads} {input} >| {output}
"""


rule split_faa:
    input:
        faa=rules.all_faa.output,
    output:
        sentinel=f"{RESULTS}/.pieces_faa/split_faa.sentinel",
    params:
        pieces=f"{RESULTS}/.pieces_faa",
        batch_size=f"{BATCH_SIZE}",
        faa_width=f"{FAA_WIDTH}",
    run:
        #!/usr/bin/env python
        from datetime import datetime
        from pathlib import Path
        from shutil import rmtree

        from Bio import SeqIO

        START = datetime.today()

        IN_FAA = Path(f"{input.faa}")
        OUT_PIECES = Path(f"{params.pieces}")
        OUT_SENTINEL = Path(f"{output.sentinel}")
        BATCH_SIZE = int(f"{params.batch_size}")
        FAA_WIDTH = int(f"{params.faa_width}")
        FORMAT = "fasta"

        # main
        in_faa = SeqIO.parse(IN_FAA, FORMAT)
        if OUT_PIECES.exists():
            rmtree(OUT_PIECES)
        OUT_PIECES.mkdir()
        batch = []
        ibatch = 0


        def write(batch, out_file, wrap=60):
            with open(out_file, "w") as hout:
                w = SeqIO.FastaIO.FastaWriter(hout, wrap=wrap)
                for seq in batch:
                    w.write_record(seq)


        for idx, seq in enumerate(in_faa):
            batch.append(seq)

            if (idx + 1) % BATCH_SIZE == 0:
                ibatch += 1
                piece = OUT_PIECES / f"{ibatch}.faa"
                write(batch, piece, FAA_WIDTH)
                batch = []

        if len(batch) > 0:
            ibatch += 1
            piece = OUT_PIECES / f"{ibatch}.faa"
            write(batch, piece, FAA_WIDTH)

        END = datetime.today()
        with open(str(OUT_SENTINEL), "w") as h_sentinel:
            h_sentinel.write(f"{END- START}\n")


rule interproscan:
    input:
        sentinel=rules.split_faa.output.sentinel,
    output:
        iscan_raw=f"{RESULTS}/.iscan_raw.tsv",
    params:
        tmp="/tmp",
        pieces=f"{RESULTS}/.pieces_iscan",
        in_dir=f"{rules.split_faa.params.pieces}",
    threads: workflow.cores
    cache: True
    run:
        import re
        import shutil
        import subprocess as sp
        import os
        from pathlib import Path

        IN_DIR = Path(f"{params.in_dir}")
        OUT_FILE = Path(f"{output}")

        OUT_DIR = Path(f"{params.pieces}")
        CPUS = int(f"{threads}")
        ISCAN_TMP = Path(f"{params.tmp}")


        def cat(*files, out_file) -> None:

            with open(out_file, "wb") as hout:
                for file in files:
                    with open(file, "rb") as hfile:
                        shutil.copyfileobj(hfile, hout)


        def gen_cmd(in_faa, out_dir):
            """
            Output formats:
            Default for protein sequences are TSV, XML and GFF3
            """
            # Handle config path: if directory, append interproscan.sh
            raw_path = config.get("interproscan", "interproscan.sh")
            # Only check if it looks like a path (contains slashes) to avoid checking system commands
            if "/" in raw_path and Path(raw_path).is_dir():
                exe = str(Path(raw_path) / "interproscan.sh")
            else:
                exe = raw_path

            cmd = [
                exe,
                "--input",
                str(in_faa),
                "--cpu",
                str(CPUS),
                "--tempdir",
                str(ISCAN_TMP),
                "--goterms",
                "--enable-tsv-residue-annot",
                "-dp",
                "-d",
                str(out_dir),
                "-f",
                "TSV",
            ]
            print(f"DEBUG: Running InterProScan command: {' '.join(cmd)}")
            return cmd

            # main


        if OUT_DIR.exists():
            shutil.rmtree(OUT_DIR)
        OUT_DIR.mkdir()
        in_faas = [
            (IN_DIR / i).resolve()
            for i in os.listdir(IN_DIR)
            if re.search(r"\d+\.faa$", i)
        ]

        for in_faa in in_faas:
            try:
                sp.run(gen_cmd(in_faa, OUT_DIR), cwd=OUT_DIR, check=True, capture_output=True, text=True)
            except sp.CalledProcessError as e:
                print(f"CRITICAL: InterProScan FAILED for {in_faa}")
                print(f"CMD: {e.cmd}")
                print("STDOUT:", e.stdout)
                print("STDERR:", e.stderr)
                raise e

        out_tsvs = [
            OUT_DIR / i for i in os.listdir(OUT_DIR) if re.search(r"\w+\.tsv$", i)
        ]

        cat(*out_tsvs, out_file=OUT_FILE)


rule add_header_iscan:
    input:
        iscan_raw=rules.interproscan.output,
    output:
        iscan=f"{RESULTS}/iscan.tsv",
    shell:
        """
# Annotate headers
workflow/scripts/add_header_iscan.R {params} {input} >| {output}
"""


rule get_archs:
    input:
        iscan=rules.add_header_iscan.output,
        neighbors=rules.get_neighbors.output,
    output:
        archs=f"{RESULTS}/archs.tsv",
    shell:
        """
workflow/scripts/archs.R {input} >| {output}
"""


rule get_absence_presence:
    input:
        taxa=rules.join_genomes_taxallnomy.output,
        proteins=rules.get_neighbors.output,
        domains=rules.get_archs.output.archs,
    output:
        absence_presence=f"{RESULTS}/absence_presence.tsv",
    shell:
        """
workflow/scripts/absence_presence.R {input} >| {output}
"""


rule get_hits:
    input:
        neighbors=rules.get_neighbors.output.neighbors,
        archs=rules.get_archs.output.archs,
    output:
        hits=f"{RESULTS}/hits.tsv",
    shell:
        """
workflow/scripts/archs_extended.R {input} >| {output}
"""




rule browser_iscan:
    input:
        iscan=f"{RESULTS}/iscan.tsv",
    output:
        db=f"{RESULTS}/browser_files/iscan.db",
    priority: 1
    shell:
        """
        python workflow/scripts/browser_iscan.py {input.iscan} {output.db}
        """



rule browser_metadata:
    input:
        metadata=f"{RESULTS}/genomes_metadata.tsv",
        # Force dependency on iscan to run sequentially
        prev=f"{RESULTS}/browser_files/iscan.db",
    output:
        db=f"{RESULTS}/browser_files/metadata.db",
    shell:
        """
        python workflow/scripts/browser_metadata.py {input.metadata} {output.db}
        """



rule browser_neighbors:
    input:
        neighbors=f"{RESULTS}/neighbors.tsv",
        # Force dependency on metadata to run sequentially
        prev=f"{RESULTS}/browser_files/metadata.db",
    output:
        db=f"{RESULTS}/browser_files/neighbors.db",
    shell:
        """
        python workflow/scripts/browser_neighbors.py {input.neighbors} {output.db}
        """
